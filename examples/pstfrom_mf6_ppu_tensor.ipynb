{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a PEST interface from MODFLOW6 using the `PstFrom` class with `PyPestUtils` for advanced pilot point parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyemu\n",
    "import flopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(\"..\",\"..\",\"pypestutils\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypestutils as ppu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An existing MODFLOW6 model is in the directory `freyberg_mf6`.  Lets check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_model_ws = os.path.join('freyberg_mf6')\n",
    "os.listdir(org_model_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all the input array and list data for this model have been written \"externally\" - this is key to using the `PstFrom` class. \n",
    "\n",
    "Let's quickly viz the model top just to remind us of what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_arr = np.loadtxt(os.path.join(org_model_ws,\"freyberg6.dis_idomain_layer3.txt\"))\n",
    "top_arr = np.loadtxt(os.path.join(org_model_ws,\"freyberg6.dis_top.txt\"))\n",
    "top_arr[id_arr==0] = np.nan\n",
    "plt.imshow(top_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's copy those files to a temporary location just to make sure we don't goof up those original files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model_ws = \"temp_pst_from_ppu\"\n",
    "if os.path.exists(tmp_model_ws):\n",
    "    shutil.rmtree(tmp_model_ws)\n",
    "shutil.copytree(org_model_ws,tmp_model_ws)\n",
    "# copy pestpp and mf6\n",
    "shutil.copy(r'C:\\bin\\modflow\\mf6.6.2_win64\\bin\\mf6.exe', os.path.join(tmp_model_ws,'mf6.exe'))\n",
    "shutil.copy(r'C:\\bin\\pestpp\\pestpp-ies.exe', os.path.join(tmp_model_ws,'pestpp-ies.exe'))\n",
    "os.listdir(tmp_model_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need just a tiny bit of info about the spatial discretization of the model - this is needed to work out separation distances between parameters for build a geostatistical prior covariance matrix later.\n",
    "\n",
    "Here we will load the flopy sim and model instance just to help us define some quantities later - flopy is not required to use the `PstFrom` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = flopy.mf6.MFSimulation.load(sim_ws=tmp_model_ws)\n",
    "m = sim.get_model(\"freyberg6\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the simple `SpatialReference` pyemu implements to help us spatially locate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = pyemu.helpers.SpatialReference.from_namfile(\n",
    "        os.path.join(tmp_model_ws, \"freyberg6.nam\"),\n",
    "        delr=m.dis.delr.array, delc=m.dis.delc.array)\n",
    "sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate a `PstFrom` class instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_ws = \"freyberg6_template\"\n",
    "pf = pyemu.utils.PstFrom(original_d=tmp_model_ws, new_d=template_ws,\n",
    "                 remove_existing=True,\n",
    "                 longnames=True, spatial_reference=sr,\n",
    "                 zero_based=False,start_datetime=\"1-1-2018\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "So now that we have a `PstFrom` instance, but its just an empty container at this point, so we need to add some PEST interface \"observations\" and \"parameters\".  Let's start with observations using MODFLOW6 head.  These are stored in `heads.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(tmp_model_ws,\"heads.csv\"),index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main entry point for adding observations is (surprise) `PstFrom.add_observations()`.  This method works on the list-type observation output file.  We need to tell it what column is the index column (can be string if there is a header or int if no header) and then what columns contain quantities we want to monitor (e.g. \"observe\") in the control file - in this case we want to monitor all columns except the index column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_df = pf.add_observations(\"heads.csv\",insfile=\"heads.csv.ins\",index_cols=\"time\",\n",
    "                    use_cols=list(df.columns.values),prefix=\"hds\",)\n",
    "hds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it returned a dataframe with lots of useful info: the observation names that were formed (`obsnme`), the values that were read from `heads.csv` (`obsval`) and also some generic weights and group names.  At this point, no control file has been created, we have simply prepared to add this observations to the control file later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".ins\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!  We also have a PEST-style instruction file for those obs.\n",
    "\n",
    "Now lets do the same for SFR observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(tmp_model_ws, \"sfr.csv\"), index_col=0)\n",
    "sfr_df = pf.add_observations(\"sfr.csv\", insfile=\"sfr.csv.ins\", index_cols=\"time\", use_cols=list(df.columns.values))\n",
    "sfr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet as!  Now that we have some observations, let's add parameters!\n",
    "\n",
    "## Pilot points and `PyPestUtils`\n",
    "\n",
    "This notebook is mostly meant to demonstrate some advanced pilot point parameterization that is possible with `PyPestUtils`, so we will only focus on HK and VK pilot point parameters.  This is just to keep the example short.  In practice, please please please parameterize boundary conditions too!\n",
    "\n",
    "We start with simple pilot points and an isotropic variogram (bearing is irrelevant when anisotropy=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pyemu.geostats.ExpVario(contribution=1.0,a=5000,bearing=0,anisotropy=1)\n",
    "pp_gs = pyemu.geostats.GeoStruct(variograms=v, transform='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_gs.plot()\n",
    "print(\"spatial variogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the idomain array to use as a zone array - this keeps us from setting up parameters in inactive model cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib = m.dis.idomain[0].array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find HK files for the upper and lower model layers (assuming model layer 2 is a semi-confining unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_arr_files = [f for f in os.listdir(tmp_model_ws) if \"npf_k_\" in f and f.endswith(\".txt\") and \"layer2\" not in f]\n",
    "hk_arr_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_file = \"freyberg6.npf_k_layer1.txt\"\n",
    "tag = arr_file.split('.')[1].replace(\"_\",\"-\")\n",
    "pf.add_parameters(filenames=arr_file,par_type=\"pilotpoints\",\n",
    "                   par_name_base=tag,pargp=tag,zone_array=ib,\n",
    "                   upper_bound=10.,lower_bound=0.1,ult_ubound=100,ult_lbound=0.01,\n",
    "                   pp_options={\"pp_space\":3},geostruct=pp_gs, transform='log')\n",
    "#let's also add the resulting hk array that modflow sees as observations\n",
    "# so we can make easy plots later...\n",
    "pf.add_observations(arr_file,prefix=tag,\n",
    "                    obsgp=tag,zone_array=ib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with how `PstFrom` has worked historically, we handed off the process to solve for the factor file (which requires solving the kriging equations for each active node) to a pure python (well, with pandas and numpy).  This was ok for toy models, but hella slow for big ugly models.  If you look at the log entries above, you should see that the instead, `PstFrom` successfully handed off the solve to `PyPestUtils`, which is exponentially faster for big models.  sweet ez! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpl_files = [f for f in os.listdir(template_ws) if f.endswith(\".tpl\")]\n",
    "tpl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(template_ws,tpl_files[0]),'r') as f:\n",
    "    for _ in range(2):\n",
    "        print(f.readline().strip())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So those might look like pretty redic parameter names, but they contain heaps of metadata to help you post process things later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So those are you standard pilot points for HK in layer 1 - same as it ever was..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geostatistical hyper-parameters using \"conceptual points\"\n",
    "\n",
    "For the HK layer 1 pilot points, we used a standard geostatistical structure - the ever popular exponential variogram.  But what if the properties that define that variogram were themselves uncertain?  Like what if the anisotropy ellipse varied in space across the model domain?  \n",
    "\n",
    "One way to tackle this is to assign hyper parameters at points, a tensor describing spatial correlation structure at each point. This makes sense when you have data (or \"geology\" = \"beer and guessing\") at various points to inform the correlation structure. We then use tensor interpolation to fill in the spatial structure where \"beer and guessing\" is missing.\n",
    "\n",
    "The field will be calculated as:\n",
    "\n",
    "field = interp_means + correlated_noise * 10**np.sqrt(interp_variances)\n",
    "\n",
    "The above gives us the spatially varying correlation structure to be convolved with white noise to give the correlated_noise, but the mean and variance of the field is also uncertain (uncertain uncertainty). The field is cacluLet's start with those using the familar pilot point scheme, although they could be added for each conceptual point instead. First describe how the mean and variance are expected to change (we will keep it simple):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_v = pyemu.geostats.ExpVario(contribution=1, a=5000)\n",
    "variance_gs = pyemu.geostats.GeoStruct(variograms=variance_v)\n",
    "mean_v = pyemu.geostats.ExpVario(contribution=1,a=5000)\n",
    "mean_gs = pyemu.geostats.GeoStruct(variograms=mean_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mean\n",
    "arr_file = \"freyberg6.npf_k33_layer3.txt\"\n",
    "tag = arr_file.split('.')[1].replace(\"_\",\"-\")\n",
    "pf.add_parameters(filenames=arr_file,par_type=\"pilotpoints\",\n",
    "                   par_name_base=tag,pargp=tag,zone_array=ib,\n",
    "                   upper_bound=10.,lower_bound=0.1,ult_ubound=100,ult_lbound=0.01,\n",
    "                   pp_options={\"pp_space\":3},geostruct=mean_gs, transform='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variance\n",
    "arr_file = \"freyberg6.npf_k33_layer3.txt\"\n",
    "tag = arr_file.split('.')[1].replace(\"_\",\"-\")\n",
    "pf.add_parameters(filenames=arr_file,par_type=\"pilotpoints\",\n",
    "                   par_name_base=tag,pargp=tag,zone_array=ib,\n",
    "                   upper_bound=10./2,lower_bound=0.1*2,ult_ubound=10,ult_lbound=0.1,\n",
    "                   pp_options={\"pp_space\":3},geostruct=mean_gs, transform='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's also add the resulting hk array that modflow sees as observations\n",
    "# so we can make easy plots later...\n",
    "pf.add_observations(arr_file,prefix=tag,\n",
    "                    obsgp=tag,zone_array=ib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the conceptual point hyper parameters from our friendly geologists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinary_kriging(cp_coords, cp_values, grid_coords,\n",
    "                     variogram_model='exponential', range_param=10000,\n",
    "                     sill=1.0, nugget=0.1, background_value=0.0,\n",
    "                     max_search_radius=1e20, min_points=1,\n",
    "                     transform=None, min_value=1e-8):\n",
    "    \"\"\"\n",
    "    Proper Ordinary Kriging with:\n",
    "    - Variogram model selection\n",
    "    - Background value fallback\n",
    "    - Search radius limitation\n",
    "    \"\"\"\n",
    "    n_grid = len(grid_coords)\n",
    "    n_control = len(cp_coords)\n",
    "    interp_values = np.full(n_grid, background_value)\n",
    "\n",
    "    # Variogram function\n",
    "    def variogram(h):\n",
    "        if variogram_model == 'exponential':\n",
    "            return nugget + (sill - nugget) * (1 - np.exp(-h / range_param))\n",
    "        elif variogram_model == 'gaussian':\n",
    "            return nugget + (sill - nugget) * (1 - np.exp(-(h ** 2) / (range_param ** 2)))\n",
    "        else:  # spherical\n",
    "            return np.where(h <= range_param,\n",
    "                            nugget + (sill - nugget) * (1.5 * h / range_param - 0.5 * (h / range_param) ** 3),\n",
    "                            sill)\n",
    "\n",
    "    for i in range(n_grid):\n",
    "        # Distances from this grid point to all control points\n",
    "        distances = np.linalg.norm(cp_coords - grid_coords[i], axis=1)\n",
    "        nearby = distances <= max_search_radius\n",
    "        n_nearby = np.sum(nearby)\n",
    "\n",
    "        if n_nearby < min_points:\n",
    "            continue  # Keeps background_value\n",
    "\n",
    "        # Build OK system: [C 1; 1^T 0] * [weights; mu] = [c; 1]\n",
    "        C = np.zeros((n_nearby + 1, n_nearby + 1))\n",
    "        c = np.zeros(n_nearby + 1)\n",
    "\n",
    "        # Fill covariance matrix\n",
    "        for j in range(n_nearby):\n",
    "            for k in range(n_nearby):\n",
    "                h = np.linalg.norm(cp_coords[nearby][j] - cp_coords[nearby][k])\n",
    "                C[j, k] = variogram(h)\n",
    "            # Last row/column (unbiasedness constraint)\n",
    "            C[j, -1] = 1\n",
    "            C[-1, j] = 1\n",
    "            # Right-hand side vector\n",
    "            h = distances[nearby][j]\n",
    "            c[j] = variogram(h)\n",
    "        c[-1] = 1  # Constraint\n",
    "\n",
    "        # Solve\n",
    "        if transform == 'log':\n",
    "            if min_value is None:\n",
    "                # Auto-choose based on data scale\n",
    "                positive_values = cp_values[cp_values > 0]\n",
    "                min_value = np.min(positive_values) * 0.01  # 1% of smallest positive value\n",
    "            cp_values_transformed = np.log10(np.maximum(cp_values, min_value))\n",
    "        else:\n",
    "            cp_values_transformed = cp_values.copy()\n",
    "\n",
    "        try:\n",
    "            weights = solve(C, c)[:-1]\n",
    "            interp_values[i] = np.sum(weights * cp_values_transformed[nearby])\n",
    "        except:\n",
    "            weights = np.exp(-distances[nearby] / range_param)\n",
    "            interp_values[i] = np.sum(weights * cp_values_transformed[nearby]) / np.sum(weights)\n",
    "\n",
    "    # After the loop, backtransform all at once:\n",
    "    if transform == 'log':\n",
    "        interp_values = 10 ** (interp_values)\n",
    "\n",
    "    return interp_values\n",
    "\n",
    "def generate_single_layer(control_points, interp_means, interp_variances, grid_coords_2d):\n",
    "    \"\"\"Generate field for a single 2D layer\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_grid = len(grid_coords_2d)\n",
    "    cp_coords = control_points[['x', 'y']].values\n",
    "    \n",
    "    # Interpolate correlation tensors\n",
    "    print(\"  Interpolating correlation tensors...\")\n",
    "    cp_tensors = create_2d_tensors(control_points)\n",
    "    interp_tensors = interpolate_tensors_2d(cp_coords, cp_tensors, grid_coords_2d)\n",
    "\n",
    "    # Generate correlated noise\n",
    "    print(\"  Generating correlated noise...\")\n",
    "    white_noise = np.random.randn(len(grid_coords_2d))\n",
    "    correlated_noise = generate_correlated_noise_2d(grid_coords_2d, interp_tensors, white_noise)\n",
    "\n",
    "    # Combine\n",
    "    field = interp_means + correlated_noise * 10**np.sqrt(interp_variances)\n",
    "\n",
    "    return field\n",
    "\n",
    "\n",
    "def generate_correlated_noise_2d(grid_coords, tensors, white_noise,\n",
    "                                 n_neighbors=50, radius_multiplier=3.0,\n",
    "                                 anisotropy_strength=1.0, debug=False):\n",
    "    \"\"\"\n",
    "    Generate spatially correlated noise respecting tensor anisotropy directions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    grid_coords : array_like, shape (n_points, 2)\n",
    "        Grid coordinates [x, y]\n",
    "    tensors : array_like, shape (n_points, 2, 2)\n",
    "        Anisotropy tensors for each grid point\n",
    "    white_noise : array_like, shape (n_points,)\n",
    "        Input white noise to be correlated\n",
    "    n_neighbors : int, default=10\n",
    "        Number of neighbors to use for correlation\n",
    "    radius_multiplier : float, default=3.0\n",
    "        Multiplier for correlation radius (not used when n_neighbors is set)\n",
    "    anisotropy_strength : float, default=2.0\n",
    "        Strength of anisotropic correlation (higher = more directional)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    correlated_noise : array_like, shape (n_points,)\n",
    "        Spatially correlated noise\n",
    "    \"\"\"\n",
    "    n_points = len(grid_coords)\n",
    "    correlated_noise = np.zeros(n_points)\n",
    "\n",
    "    for i in range(n_points):\n",
    "        # Diagonalize tensor to get principal axes and correlation lengths\n",
    "        evals, evecs = np.linalg.eigh(tensors[i])\n",
    "        if evals[1] < evals[0]:\n",
    "            evals, evecs = evals[::-1], evecs[:, ::-1]\n",
    "\n",
    "        # some debuggery\n",
    "        if debug:\n",
    "            print(f\"Eigenvalue 0: {evals[0]}\")\n",
    "            print(f\"Eigenvector 0: {evecs[:, 0]}\")  # First COLUMN\n",
    "            print(f\"Eigenvalue 1: {evals[1]}\")\n",
    "            print(f\"Eigenvector 1: {evecs[:, 1]}\")  # Second COLUMN\n",
    "            # Find which eigenvalue is larger\n",
    "            max_idx = np.argmax(np.abs(evals))\n",
    "            principal_direction = evecs[:, max_idx]\n",
    "            print(f\"Principal direction vector: {principal_direction}\")\n",
    "            angle = np.arctan2(principal_direction[1], principal_direction[0])\n",
    "            angle_degrees = np.degrees(angle)\n",
    "            print(f\"angle: {angle_degrees}\")\n",
    "\n",
    "        major_corr_length = np.sqrt(evals[1])  # Length along major axis\n",
    "        minor_corr_length = np.sqrt(evals[0])  # Length along minor axis\n",
    "\n",
    "        # Create anisotropic transform\n",
    "        # Stretch major axis to make correlation stronger in that direction\n",
    "        scale_for_minor_axis = minor_corr_length / anisotropy_strength  # Compress minor\n",
    "        scale_for_major_axis = major_corr_length * anisotropy_strength  # Stretch major\n",
    "\n",
    "        transform = evecs @ np.diag([scale_for_minor_axis, scale_for_major_axis])\n",
    "\n",
    "        # Transform all relative positions to anisotropic space\n",
    "        all_dx = grid_coords - grid_coords[i]\n",
    "        inv_transform = np.linalg.inv(transform)\n",
    "        dx_transformed = all_dx @ inv_transform.T\n",
    "        aniso_distances = np.linalg.norm(dx_transformed, axis=1)\n",
    "        actual_distances = np.linalg.norm(all_dx, axis=1)\n",
    "\n",
    "        # Select neighbors based on anisotropic distance, closer in warped space\n",
    "        if n_neighbors:\n",
    "            neighbor_indices = np.argsort(aniso_distances)[:min(n_neighbors, n_points)]\n",
    "        else:\n",
    "            max_dist = radius_multiplier * major_corr_length\n",
    "            neighbor_indices = np.where(aniso_distances <= max_dist)[0]\n",
    "\n",
    "        # Compute correlation weights using anisotropic distances\n",
    "        weighted_sum = 0.0\n",
    "        sum_weights = 0.0\n",
    "\n",
    "        for j in neighbor_indices:\n",
    "            # Use major correlation length as decay scale\n",
    "            weight = np.exp(-(actual_distances[j]/scale_for_major_axis)**2)\n",
    "            weighted_sum += weight * white_noise[j]\n",
    "            sum_weights += weight\n",
    "\n",
    "        # Normalize\n",
    "        if sum_weights > 1e-10:\n",
    "            correlated_noise[i] = weighted_sum / sum_weights\n",
    "        else:\n",
    "            correlated_noise[i] = white_noise[i]\n",
    "\n",
    "    return correlated_noise\n",
    "\n",
    "def create_2d_tensors(control_points):\n",
    "    # Fixed tensor creation (swap major/minor in diagonal matrix)\n",
    "    n = len(control_points)\n",
    "    tensors = np.zeros((n, 2, 2))\n",
    "\n",
    "    for i in range(n):\n",
    "        major = control_points.iloc[i]['major']\n",
    "        minor = control_points.iloc[i]['major'] / control_points.iloc[i]['ratio']\n",
    "        bearing = control_points.iloc[i]['bearing'] # - 90 # assume bearing has N=0\n",
    "\n",
    "        theta = np.radians(bearing)\n",
    "        R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                      [np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "        # Fixed: Put minor first, major second\n",
    "        S = np.diag([minor ** 2, major ** 2])\n",
    "        tensors[i] = R @ S @ R.T\n",
    "\n",
    "    return tensors\n",
    "\n",
    "\n",
    "def interpolate_tensors_2d(cp_coords, cp_tensors, grid_coords):\n",
    "    \"\"\"Interpolate 2x2 tensors using log-Euclidean approach\"\"\"    \n",
    "    from scipy.linalg import logm, expm, solve\n",
    "    n_grid = len(grid_coords)\n",
    "    interp_tensors = np.zeros((n_grid, 2, 2))\n",
    "\n",
    "    # Convert to log space\n",
    "    log_tensors = np.array([logm(t) for t in cp_tensors])\n",
    "\n",
    "    # Interpolate each component\n",
    "    for i in range(2):\n",
    "        for j in range(i, 2):\n",
    "            values = log_tensors[:, i, j].real\n",
    "            bg_value = np.mean(values)  # Use mean of control points\n",
    "            interp_values = ordinary_kriging(cp_coords, values, grid_coords, \n",
    "                                       background_value=bg_value)\n",
    "            # interp_values = exponential_variogram_interpolate(cp_coords, values, grid_coords)\n",
    "            # interp_values = idw_interpolate(cp_coords, values, grid_coords)\n",
    "            interp_tensors[:, i, j] = interp_values\n",
    "            if i != j:\n",
    "                interp_tensors[:, j, i] = interp_values\n",
    "\n",
    "    # Convert back from log space\n",
    "    for i in range(n_grid):\n",
    "        interp_tensors[i] = expm(interp_tensors[i])\n",
    "\n",
    "    # # Basic ellipse visualization (most intuitive)\n",
    "    # fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    # visualize_tensors_as_ellipses(grid_coords, interp_tensors,\n",
    "    #                               subsample=20,  # Plot every 20th tensor\n",
    "    #                               scale_factor=0.5,\n",
    "    #                               ax=ax)\n",
    "\n",
    "    # # Complete analysis\n",
    "    # plot_tensor_field_analysis(grid_coords, interp_tensors, cp_coords)\n",
    "\n",
    "    return interp_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_means = np.loadtxt(os.path.join(org_model_ws,arr_file)).flatten()\n",
    "interp_variances = np.loadtxt(os.path.join(org_model_ws,arr_file)).flatten()/10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_centers = m.modelgrid.xcellcenters.flatten()\n",
    "y_centers = m.modelgrid.ycellcenters.flatten()\n",
    "grid_coords = np.column_stack([x_centers,y_centers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptual_points = pd.DataFrame({'x': [3, 3, 3, 3, 5, 7, 15, 20, 25, 35],\n",
    "                                  'y': [3, 5, 10, 15, 5, 12, 10, 15, 3, 10],\n",
    "                                  'bearing': [-40, -30, -20, -20, -30, -40, 0, 0, 10 ,200],\n",
    "                                  'major': [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 2000],\n",
    "                                  'ratio': [5, 5, 2, 2, 2, 2, 2, 5, 5, 2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptual_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = generate_single_layer(conceptual_points, interp_means, interp_variances, grid_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(field.reshape(40,20))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the control file, pest interface files, and forward run script\n",
    "At this point, we have some parameters and some observations, so we can create a control file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.mod_sys_cmds.append(\"mf6\")\n",
    "pf.pre_py_cmds.insert(0,\"import sys\")\n",
    "pf.pre_py_cmds.insert(1,\"sys.path.append(os.path.join('..','..','..','pypestutils'))\")\n",
    "pst = pf.build_pst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [print(line.rstrip()) for line in open(os.path.join(template_ws,\"forward_run.py\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting initial parameter bounds and values\n",
    "\n",
    "Here is some gory detail regarding defining the hyper parameters for both layer 3 HK and layer 2 VK..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the initial and bounds for the fill values\n",
    "par = pst.parameter_data\n",
    "\n",
    "apar = par.loc[par.pname.str.contains(\"aniso\"),:]\n",
    "bpar = par.loc[par.pname.str.contains(\"bearing\"), :]\n",
    "par.loc[apar.parnme.str.contains(\"layer3\").index,\"parval1\"] = 3\n",
    "par.loc[apar.parnme.str.contains(\"layer3\").index,\"parlbnd\"] = 1\n",
    "par.loc[apar.parnme.str.contains(\"layer3\").index,\"parubnd\"] = 5\n",
    "\n",
    "par.loc[apar.parnme.str.contains(\"layer2\").index,\"parval1\"] = 2\n",
    "par.loc[apar.parnme.str.contains(\"layer2\").index,\"parlbnd\"] = 0\n",
    "par.loc[apar.parnme.str.contains(\"layer2\").index,\"parubnd\"] = 4\n",
    "\n",
    "par.loc[bpar.parnme.str.contains(\"layer3\").index,\"parval1\"] = 0\n",
    "par.loc[bpar.parnme.str.contains(\"layer3\").index,\"parlbnd\"] = -20\n",
    "par.loc[bpar.parnme.str.contains(\"layer3\").index,\"parubnd\"] = 20\n",
    "\n",
    "par.loc[bpar.parnme.str.contains(\"layer2\").index,\"parval1\"] = 0\n",
    "par.loc[bpar.parnme.str.contains(\"layer2\").index,\"parlbnd\"] = 20\n",
    "par.loc[bpar.parnme.str.contains(\"layer2\").index,\"parubnd\"] = 40\n",
    "\n",
    "cat1par = par.loc[par.apply(lambda x: x.threshcat==\"0\" and x.usecol==\"threshfill\",axis=1),\"parnme\"]\n",
    "cat2par = par.loc[par.apply(lambda x: x.threshcat == \"1\" and x.usecol == \"threshfill\", axis=1), \"parnme\"]\n",
    "assert cat1par.shape[0] == 1\n",
    "assert cat2par.shape[0] == 1\n",
    "\n",
    "cat1parvk = [p for p in cat1par if \"k:1\" in p]\n",
    "cat2parvk = [p for p in cat2par if \"k:1\" in p]\n",
    "for lst in [cat2parvk,cat1parvk]:\n",
    "    assert len(lst) > 0\n",
    "\n",
    "#these are the values that will fill the two categories of VK - \n",
    "# one is low (clay) and one is high (sand - the windows)\n",
    "par.loc[cat1parvk, \"parval1\"] = 0.0001\n",
    "par.loc[cat1parvk, \"parubnd\"] = 0.01\n",
    "par.loc[cat1parvk, \"parlbnd\"] = 0.000001\n",
    "par.loc[cat1parvk, \"partrans\"] = \"log\"\n",
    "par.loc[cat2parvk, \"parval1\"] = 0.1\n",
    "par.loc[cat2parvk, \"parubnd\"] = 1\n",
    "par.loc[cat2parvk, \"parlbnd\"] = 0.01\n",
    "par.loc[cat2parvk, \"partrans\"] = \"log\"\n",
    "\n",
    "\n",
    "cat1par = par.loc[par.apply(lambda x: x.threshcat == \"0\" and x.usecol == \"threshproportion\", axis=1), \"parnme\"]\n",
    "cat2par = par.loc[par.apply(lambda x: x.threshcat == \"1\" and x.usecol == \"threshproportion\", axis=1), \"parnme\"]\n",
    "\n",
    "assert cat1par.shape[0] == 1\n",
    "assert cat2par.shape[0] == 1\n",
    "\n",
    "#these are the proportions of clay and sand in the resulting categorical array\n",
    "#really under the hood, only the first one is used, so we can fix the other.\n",
    "par.loc[cat1par, \"parval1\"] = 0.95\n",
    "par.loc[cat1par, \"parubnd\"] = 1.0\n",
    "par.loc[cat1par, \"parlbnd\"] = 0.9\n",
    "par.loc[cat1par,\"partrans\"] = \"none\"\n",
    "\n",
    "# since the apply method only looks that first proportion, we can just fix this one\n",
    "par.loc[cat2par, \"parval1\"] = 1\n",
    "par.loc[cat2par, \"parubnd\"] = 1\n",
    "par.loc[cat2par, \"parlbnd\"] = 1\n",
    "par.loc[cat2par,\"partrans\"] = \"fixed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a prior parameter ensemble, then run and viz a real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(122346)\n",
    "pe = pf.draw(num_reals=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.to_csv(os.path.join(template_ws,\"prior.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = 0\n",
    "pst_name = \"real_{0}.pst\".format(real)\n",
    "pst.parameter_data.loc[pst.adj_par_names,\"parval1\"] = pe.loc[real,pst.adj_par_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = 0\n",
    "pst.write(os.path.join(pf.new_d,pst_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.run(\"pestpp-ies {0}\".format(pst_name),cwd=pf.new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.set_res(os.path.join(pf.new_d,pst_name.replace(\".pst\",\".base.rei\")))\n",
    "res = pst.res\n",
    "obs = pst.observation_data\n",
    "grps = [o for o in obs.obgnme.unique() if o.startswith(\"npf\") and \"result\" not in o and \"aniso\" not in o]\n",
    "grps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gobs = obs.loc[obs.obgnme.isin(grps),:].copy()\n",
    "gobs[\"i\"] = gobs.i.astype(int)\n",
    "gobs[\"j\"] = gobs.j.astype(int)\n",
    "gobs[\"k\"] = gobs.obgnme.apply(lambda x: int(x.split('-')[2].replace(\"layer\",\"\")) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk = gobs.k.unique()\n",
    "uk.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in uk:\n",
    "    kobs = gobs.loc[gobs.k==k,:]\n",
    "    ug = kobs.obgnme.unique()\n",
    "    ug.sort()\n",
    "    fig,axes = plt.subplots(1,4,figsize=(20,6))\n",
    "    axes = np.atleast_1d(axes)\n",
    "    for ax in axes:\n",
    "        ax.set_frame_on(False)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([])\n",
    "    for g,ax in zip(ug,axes):\n",
    "        gkobs = kobs.loc[kobs.obgnme==g,:]\n",
    "        \n",
    "        arr = np.zeros_like(top_arr)\n",
    "        arr[gkobs.i,gkobs.j] = res.loc[gkobs.obsnme,\"modelled\"].values\n",
    "        ax.set_aspect(\"equal\")\n",
    "        label = \"\"\n",
    "        if \"bearing\" not in g and \"aniso\" not in g:\n",
    "            arr = np.log10(arr)\n",
    "            label = \"$log_{10}$\"\n",
    "        cb = ax.imshow(arr)\n",
    "        plt.colorbar(cb,ax=ax,label=label)\n",
    "        ax.set_title(\"layer: {0} group: {1}\".format(k+1,g),loc=\"left\",fontsize=15)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stunning isn't it?!  There is clearly a lot subjectivity in the form of defining the prior for the hyper parameters required to use these non-stationary geostats, but they do afford more opportunities to express (stochastic) expert knowledge.  To be honest, there was a lot of experimenting with this notebook to get these figures to look this way - playing with variograms and parameter initial values and bounds a lot.  You encouraged to do the same!  scroll back up, change things, and \"restart kernel and run all\" - this will help build some better intution, promise...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

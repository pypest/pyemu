{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced non-stationary geostatistical parameterization using `pyemu.utils.nsaf_utils`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wesk\\AppData\\Local\\miniforge3\\envs\\p312\\Lib\\site-packages\\IPython\\core\\pylabtools.py:77: DeprecationWarning: backend2gui is deprecated since IPython 8.24, backends are managed in matplotlib and can be externally registered.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pyemu\n",
    "import flopy\n",
    "from pyemu.utils import nsaf_utils as nsaf\n",
    "from pyemu.plot import plot_utils as pu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move this lower if still wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(\"..\",\"..\",\"pypestutils\"))\n",
    "import pypestutils as ppu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An existing MODFLOW6 model is in the directory `freyberg_mf6`.  Lets check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['freyberg6.dis',\n",
       " 'freyberg6.dis.grb',\n",
       " 'freyberg6.dis_botm_layer1.txt',\n",
       " 'freyberg6.dis_botm_layer2.txt',\n",
       " 'freyberg6.dis_botm_layer3.txt',\n",
       " 'freyberg6.dis_delc.txt',\n",
       " 'freyberg6.dis_delr.txt',\n",
       " 'freyberg6.dis_idomain_layer1.txt',\n",
       " 'freyberg6.dis_idomain_layer2.txt',\n",
       " 'freyberg6.dis_idomain_layer3.txt',\n",
       " 'freyberg6.dis_top.txt',\n",
       " 'freyberg6.ghb',\n",
       " 'freyberg6.ghb_stress_period_data_1.txt',\n",
       " 'freyberg6.ic',\n",
       " 'freyberg6.ic_strt_layer1.txt',\n",
       " 'freyberg6.ic_strt_layer2.txt',\n",
       " 'freyberg6.ic_strt_layer3.txt',\n",
       " 'freyberg6.ims',\n",
       " 'freyberg6.lst',\n",
       " 'freyberg6.nam',\n",
       " 'freyberg6.npf',\n",
       " 'freyberg6.npf_icelltype_layer1.txt',\n",
       " 'freyberg6.npf_icelltype_layer2.txt',\n",
       " 'freyberg6.npf_icelltype_layer3.txt',\n",
       " 'freyberg6.npf_k33_layer1.txt',\n",
       " 'freyberg6.npf_k33_layer2.txt',\n",
       " 'freyberg6.npf_k33_layer3.txt',\n",
       " 'freyberg6.npf_k_layer1.txt',\n",
       " 'freyberg6.npf_k_layer2.txt',\n",
       " 'freyberg6.npf_k_layer3.txt',\n",
       " 'freyberg6.oc',\n",
       " 'freyberg6.rch',\n",
       " 'freyberg6.rch_recharge_1.txt',\n",
       " 'freyberg6.rch_recharge_10.txt',\n",
       " 'freyberg6.rch_recharge_11.txt',\n",
       " 'freyberg6.rch_recharge_12.txt',\n",
       " 'freyberg6.rch_recharge_13.txt',\n",
       " 'freyberg6.rch_recharge_14.txt',\n",
       " 'freyberg6.rch_recharge_15.txt',\n",
       " 'freyberg6.rch_recharge_16.txt',\n",
       " 'freyberg6.rch_recharge_17.txt',\n",
       " 'freyberg6.rch_recharge_18.txt',\n",
       " 'freyberg6.rch_recharge_19.txt',\n",
       " 'freyberg6.rch_recharge_2.txt',\n",
       " 'freyberg6.rch_recharge_20.txt',\n",
       " 'freyberg6.rch_recharge_21.txt',\n",
       " 'freyberg6.rch_recharge_22.txt',\n",
       " 'freyberg6.rch_recharge_23.txt',\n",
       " 'freyberg6.rch_recharge_24.txt',\n",
       " 'freyberg6.rch_recharge_25.txt',\n",
       " 'freyberg6.rch_recharge_3.txt',\n",
       " 'freyberg6.rch_recharge_4.txt',\n",
       " 'freyberg6.rch_recharge_5.txt',\n",
       " 'freyberg6.rch_recharge_6.txt',\n",
       " 'freyberg6.rch_recharge_7.txt',\n",
       " 'freyberg6.rch_recharge_8.txt',\n",
       " 'freyberg6.rch_recharge_9.txt',\n",
       " 'freyberg6.sfr',\n",
       " 'freyberg6.sfr_connectiondata.txt',\n",
       " 'freyberg6.sfr_packagedata.txt',\n",
       " 'freyberg6.sfr_perioddata_1.txt',\n",
       " 'freyberg6.sfr_perioddata_10.txt',\n",
       " 'freyberg6.sfr_perioddata_11.txt',\n",
       " 'freyberg6.sfr_perioddata_12.txt',\n",
       " 'freyberg6.sfr_perioddata_13.txt',\n",
       " 'freyberg6.sfr_perioddata_14.txt',\n",
       " 'freyberg6.sfr_perioddata_15.txt',\n",
       " 'freyberg6.sfr_perioddata_16.txt',\n",
       " 'freyberg6.sfr_perioddata_17.txt',\n",
       " 'freyberg6.sfr_perioddata_18.txt',\n",
       " 'freyberg6.sfr_perioddata_19.txt',\n",
       " 'freyberg6.sfr_perioddata_2.txt',\n",
       " 'freyberg6.sfr_perioddata_20.txt',\n",
       " 'freyberg6.sfr_perioddata_21.txt',\n",
       " 'freyberg6.sfr_perioddata_22.txt',\n",
       " 'freyberg6.sfr_perioddata_23.txt',\n",
       " 'freyberg6.sfr_perioddata_24.txt',\n",
       " 'freyberg6.sfr_perioddata_25.txt',\n",
       " 'freyberg6.sfr_perioddata_3.txt',\n",
       " 'freyberg6.sfr_perioddata_4.txt',\n",
       " 'freyberg6.sfr_perioddata_5.txt',\n",
       " 'freyberg6.sfr_perioddata_6.txt',\n",
       " 'freyberg6.sfr_perioddata_7.txt',\n",
       " 'freyberg6.sfr_perioddata_8.txt',\n",
       " 'freyberg6.sfr_perioddata_9.txt',\n",
       " 'freyberg6.sto',\n",
       " 'freyberg6.sto_iconvert_layer1.txt',\n",
       " 'freyberg6.sto_iconvert_layer2.txt',\n",
       " 'freyberg6.sto_iconvert_layer3.txt',\n",
       " 'freyberg6.sto_ss_layer1.txt',\n",
       " 'freyberg6.sto_ss_layer2.txt',\n",
       " 'freyberg6.sto_ss_layer3.txt',\n",
       " 'freyberg6.sto_sy_layer1.txt',\n",
       " 'freyberg6.sto_sy_layer2.txt',\n",
       " 'freyberg6.sto_sy_layer3.txt',\n",
       " 'freyberg6.tdis',\n",
       " 'freyberg6.wel',\n",
       " 'freyberg6.wel_stress_period_data_1.txt',\n",
       " 'freyberg6.wel_stress_period_data_10.txt',\n",
       " 'freyberg6.wel_stress_period_data_11.txt',\n",
       " 'freyberg6.wel_stress_period_data_12.txt',\n",
       " 'freyberg6.wel_stress_period_data_13.txt',\n",
       " 'freyberg6.wel_stress_period_data_14.txt',\n",
       " 'freyberg6.wel_stress_period_data_15.txt',\n",
       " 'freyberg6.wel_stress_period_data_16.txt',\n",
       " 'freyberg6.wel_stress_period_data_17.txt',\n",
       " 'freyberg6.wel_stress_period_data_18.txt',\n",
       " 'freyberg6.wel_stress_period_data_19.txt',\n",
       " 'freyberg6.wel_stress_period_data_2.txt',\n",
       " 'freyberg6.wel_stress_period_data_20.txt',\n",
       " 'freyberg6.wel_stress_period_data_21.txt',\n",
       " 'freyberg6.wel_stress_period_data_22.txt',\n",
       " 'freyberg6.wel_stress_period_data_23.txt',\n",
       " 'freyberg6.wel_stress_period_data_24.txt',\n",
       " 'freyberg6.wel_stress_period_data_25.txt',\n",
       " 'freyberg6.wel_stress_period_data_3.txt',\n",
       " 'freyberg6.wel_stress_period_data_4.txt',\n",
       " 'freyberg6.wel_stress_period_data_5.txt',\n",
       " 'freyberg6.wel_stress_period_data_6.txt',\n",
       " 'freyberg6.wel_stress_period_data_7.txt',\n",
       " 'freyberg6.wel_stress_period_data_8.txt',\n",
       " 'freyberg6.wel_stress_period_data_9.txt',\n",
       " 'freyberg6_freyberg.cbc',\n",
       " 'freyberg6_freyberg.hds',\n",
       " 'head.obs',\n",
       " 'heads.csv',\n",
       " 'mfsim.lst',\n",
       " 'mfsim.nam',\n",
       " 'sfr.csv',\n",
       " 'sfr.obs']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_model_ws = os.path.join('freyberg_mf6')\n",
    "os.listdir(org_model_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all the input array and list data for this model have been written \"externally\" - this is key to using the `PstFrom` class. \n",
    "\n",
    "Let's quickly viz the model top just to remind us of what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wesk\\AppData\\Local\\miniforge3\\envs\\p312\\Lib\\site-packages\\IPython\\core\\pylabtools.py:77: DeprecationWarning: backend2gui is deprecated since IPython 8.24, backends are managed in matplotlib and can be externally registered.\n",
      "C:\\Users\\wesk\\AppData\\Local\\miniforge3\\envs\\p312\\Lib\\site-packages\\IPython\\core\\pylabtools.py:77: DeprecationWarning: backend2gui is deprecated since IPython 8.24, backends are managed in matplotlib and can be externally registered.\n"
     ]
    }
   ],
   "source": [
    "id_arr = np.loadtxt(os.path.join(org_model_ws,\"freyberg6.dis_idomain_layer3.txt\"))\n",
    "top_arr = np.loadtxt(os.path.join(org_model_ws,\"freyberg6.dis_top.txt\"))\n",
    "top_arr[id_arr==0] = np.nan\n",
    "plt.imshow(top_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's copy those files to a temporary location just to make sure we don't goof up those original files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['freyberg6.dis',\n",
       " 'freyberg6.dis.grb',\n",
       " 'freyberg6.dis_botm_layer1.txt',\n",
       " 'freyberg6.dis_botm_layer2.txt',\n",
       " 'freyberg6.dis_botm_layer3.txt',\n",
       " 'freyberg6.dis_delc.txt',\n",
       " 'freyberg6.dis_delr.txt',\n",
       " 'freyberg6.dis_idomain_layer1.txt',\n",
       " 'freyberg6.dis_idomain_layer2.txt',\n",
       " 'freyberg6.dis_idomain_layer3.txt',\n",
       " 'freyberg6.dis_top.txt',\n",
       " 'freyberg6.ghb',\n",
       " 'freyberg6.ghb_stress_period_data_1.txt',\n",
       " 'freyberg6.ic',\n",
       " 'freyberg6.ic_strt_layer1.txt',\n",
       " 'freyberg6.ic_strt_layer2.txt',\n",
       " 'freyberg6.ic_strt_layer3.txt',\n",
       " 'freyberg6.ims',\n",
       " 'freyberg6.lst',\n",
       " 'freyberg6.nam',\n",
       " 'freyberg6.npf',\n",
       " 'freyberg6.npf_icelltype_layer1.txt',\n",
       " 'freyberg6.npf_icelltype_layer2.txt',\n",
       " 'freyberg6.npf_icelltype_layer3.txt',\n",
       " 'freyberg6.npf_k33_layer1.txt',\n",
       " 'freyberg6.npf_k33_layer2.txt',\n",
       " 'freyberg6.npf_k33_layer3.txt',\n",
       " 'freyberg6.npf_k_layer1.txt',\n",
       " 'freyberg6.npf_k_layer2.txt',\n",
       " 'freyberg6.npf_k_layer3.txt',\n",
       " 'freyberg6.oc',\n",
       " 'freyberg6.rch',\n",
       " 'freyberg6.rch_recharge_1.txt',\n",
       " 'freyberg6.rch_recharge_10.txt',\n",
       " 'freyberg6.rch_recharge_11.txt',\n",
       " 'freyberg6.rch_recharge_12.txt',\n",
       " 'freyberg6.rch_recharge_13.txt',\n",
       " 'freyberg6.rch_recharge_14.txt',\n",
       " 'freyberg6.rch_recharge_15.txt',\n",
       " 'freyberg6.rch_recharge_16.txt',\n",
       " 'freyberg6.rch_recharge_17.txt',\n",
       " 'freyberg6.rch_recharge_18.txt',\n",
       " 'freyberg6.rch_recharge_19.txt',\n",
       " 'freyberg6.rch_recharge_2.txt',\n",
       " 'freyberg6.rch_recharge_20.txt',\n",
       " 'freyberg6.rch_recharge_21.txt',\n",
       " 'freyberg6.rch_recharge_22.txt',\n",
       " 'freyberg6.rch_recharge_23.txt',\n",
       " 'freyberg6.rch_recharge_24.txt',\n",
       " 'freyberg6.rch_recharge_25.txt',\n",
       " 'freyberg6.rch_recharge_3.txt',\n",
       " 'freyberg6.rch_recharge_4.txt',\n",
       " 'freyberg6.rch_recharge_5.txt',\n",
       " 'freyberg6.rch_recharge_6.txt',\n",
       " 'freyberg6.rch_recharge_7.txt',\n",
       " 'freyberg6.rch_recharge_8.txt',\n",
       " 'freyberg6.rch_recharge_9.txt',\n",
       " 'freyberg6.sfr',\n",
       " 'freyberg6.sfr_connectiondata.txt',\n",
       " 'freyberg6.sfr_packagedata.txt',\n",
       " 'freyberg6.sfr_perioddata_1.txt',\n",
       " 'freyberg6.sfr_perioddata_10.txt',\n",
       " 'freyberg6.sfr_perioddata_11.txt',\n",
       " 'freyberg6.sfr_perioddata_12.txt',\n",
       " 'freyberg6.sfr_perioddata_13.txt',\n",
       " 'freyberg6.sfr_perioddata_14.txt',\n",
       " 'freyberg6.sfr_perioddata_15.txt',\n",
       " 'freyberg6.sfr_perioddata_16.txt',\n",
       " 'freyberg6.sfr_perioddata_17.txt',\n",
       " 'freyberg6.sfr_perioddata_18.txt',\n",
       " 'freyberg6.sfr_perioddata_19.txt',\n",
       " 'freyberg6.sfr_perioddata_2.txt',\n",
       " 'freyberg6.sfr_perioddata_20.txt',\n",
       " 'freyberg6.sfr_perioddata_21.txt',\n",
       " 'freyberg6.sfr_perioddata_22.txt',\n",
       " 'freyberg6.sfr_perioddata_23.txt',\n",
       " 'freyberg6.sfr_perioddata_24.txt',\n",
       " 'freyberg6.sfr_perioddata_25.txt',\n",
       " 'freyberg6.sfr_perioddata_3.txt',\n",
       " 'freyberg6.sfr_perioddata_4.txt',\n",
       " 'freyberg6.sfr_perioddata_5.txt',\n",
       " 'freyberg6.sfr_perioddata_6.txt',\n",
       " 'freyberg6.sfr_perioddata_7.txt',\n",
       " 'freyberg6.sfr_perioddata_8.txt',\n",
       " 'freyberg6.sfr_perioddata_9.txt',\n",
       " 'freyberg6.sto',\n",
       " 'freyberg6.sto_iconvert_layer1.txt',\n",
       " 'freyberg6.sto_iconvert_layer2.txt',\n",
       " 'freyberg6.sto_iconvert_layer3.txt',\n",
       " 'freyberg6.sto_ss_layer1.txt',\n",
       " 'freyberg6.sto_ss_layer2.txt',\n",
       " 'freyberg6.sto_ss_layer3.txt',\n",
       " 'freyberg6.sto_sy_layer1.txt',\n",
       " 'freyberg6.sto_sy_layer2.txt',\n",
       " 'freyberg6.sto_sy_layer3.txt',\n",
       " 'freyberg6.tdis',\n",
       " 'freyberg6.wel',\n",
       " 'freyberg6.wel_stress_period_data_1.txt',\n",
       " 'freyberg6.wel_stress_period_data_10.txt',\n",
       " 'freyberg6.wel_stress_period_data_11.txt',\n",
       " 'freyberg6.wel_stress_period_data_12.txt',\n",
       " 'freyberg6.wel_stress_period_data_13.txt',\n",
       " 'freyberg6.wel_stress_period_data_14.txt',\n",
       " 'freyberg6.wel_stress_period_data_15.txt',\n",
       " 'freyberg6.wel_stress_period_data_16.txt',\n",
       " 'freyberg6.wel_stress_period_data_17.txt',\n",
       " 'freyberg6.wel_stress_period_data_18.txt',\n",
       " 'freyberg6.wel_stress_period_data_19.txt',\n",
       " 'freyberg6.wel_stress_period_data_2.txt',\n",
       " 'freyberg6.wel_stress_period_data_20.txt',\n",
       " 'freyberg6.wel_stress_period_data_21.txt',\n",
       " 'freyberg6.wel_stress_period_data_22.txt',\n",
       " 'freyberg6.wel_stress_period_data_23.txt',\n",
       " 'freyberg6.wel_stress_period_data_24.txt',\n",
       " 'freyberg6.wel_stress_period_data_25.txt',\n",
       " 'freyberg6.wel_stress_period_data_3.txt',\n",
       " 'freyberg6.wel_stress_period_data_4.txt',\n",
       " 'freyberg6.wel_stress_period_data_5.txt',\n",
       " 'freyberg6.wel_stress_period_data_6.txt',\n",
       " 'freyberg6.wel_stress_period_data_7.txt',\n",
       " 'freyberg6.wel_stress_period_data_8.txt',\n",
       " 'freyberg6.wel_stress_period_data_9.txt',\n",
       " 'freyberg6_freyberg.cbc',\n",
       " 'freyberg6_freyberg.hds',\n",
       " 'head.obs',\n",
       " 'heads.csv',\n",
       " 'mf6.exe',\n",
       " 'mfsim.lst',\n",
       " 'mfsim.nam',\n",
       " 'pestpp-ies.exe',\n",
       " 'sfr.csv',\n",
       " 'sfr.obs']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model_ws = \"temp_pst_from_nsaf\"\n",
    "if os.path.exists(tmp_model_ws):\n",
    "    shutil.rmtree(tmp_model_ws)\n",
    "shutil.copytree(org_model_ws,tmp_model_ws)\n",
    "# copy pestpp and mf6\n",
    "shutil.copy(r'C:\\bin\\modflow\\mf6.6.2_win64\\bin\\mf6.exe', os.path.join(tmp_model_ws,'mf6.exe'))\n",
    "shutil.copy(r'C:\\bin\\pestpp\\pestpp-ies.exe', os.path.join(tmp_model_ws,'pestpp-ies.exe'))\n",
    "os.listdir(tmp_model_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will load the flopy sim and model instance just to help us define some quantities later - flopy is not required to use the `PstFrom` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading simulation...\n",
      "  loading simulation name file...\n",
      "  loading tdis package...\n",
      "  loading model gwf6...\n",
      "    loading package dis...\n",
      "    loading package ic...\n",
      "    loading package npf...\n",
      "    loading package sto...\n",
      "    loading package oc...\n",
      "    loading package wel...\n",
      "    loading package rch...\n",
      "    loading package ghb...\n",
      "    loading package sfr...\n",
      "    loading package obs...\n",
      "  loading solution package freyberg6...\n"
     ]
    }
   ],
   "source": [
    "sim = flopy.mf6.MFSimulation.load(sim_ws=tmp_model_ws)\n",
    "m = sim.get_model(\"freyberg6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will need dataframe mapping cell indicies to coordinates later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = m.modelgrid.xyzcellcenters\n",
    "grid_coords = pd.DataFrame({\n",
    "    'x': np.tile(cc[0].flatten(), cc[2].shape[0]),  # Repeat x,y for each layer\n",
    "    'y': np.tile(cc[1].flatten(), cc[2].shape[0]), \n",
    "    'z': cc[2].flatten()  # All z values\n",
    "})\n",
    "grid_coords_2d = pd.DataFrame({\n",
    "    'x': cc[0].flatten(),  # Just x coordinates (no tiling)\n",
    "    'y': cc[1].flatten(),  # Just y coordinates (no tiling)\n",
    "}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and we will define a zone from rows 30 to 40 for use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = m.dis.idomain.array.copy()\n",
    "zones[:,30:,:] = np.where(zones[:,30:,:]==1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15f977c3fb0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(zones[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use the term \"conceptual points\" for points describing the anisotropic tensor of spatial correlation. We use a \"geological convention\" where: \n",
    "1) \"bearing\" describes the orientation of the major axis with 0 degrees equal to north\n",
    "    a) a positive bearing represents the clockwise rotation of the major axis\n",
    "2) \"transverse\" axis is perpendicular to the major axis and always in the horizontal plane\n",
    "3) \"dip\" is the rotation of the major axis out of the horizontal plane with positive dip indicating clockwise rotation around the \"transverse\" axis (i.e., downward rotation in the direction of bearing)\n",
    "4) \"normal\" axis is perpendicular to the \"major\" and \"transverse\" axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A full description of the anisotropy tensor requires 5 parameters\n",
    "1) magnitude of the major axis (originally horizontal before rotation by \"dip\")\n",
    "2) magnitude of the transverse axis (always horizontal by convention)\n",
    "3) magnitude of the normal axis (originally vertical before rotation by \"dip\")\n",
    "4) bearing of the major axis\n",
    "5) dip of the major axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets define some conceptual points for the Freyberg, by row and col, and propagate to all layers.\n",
    "Imagine this is a fluvial system with higher correlation lengths in the direction of the stream channel.\n",
    "We imagine a couple streams coming in along the top of the model:\n",
    "    one from the NW with a bearing of 100\n",
    "    one from the NE with a vearing of 200 (or -160)\n",
    "We also imagine a small fluvial fan coming off the bedrock (inactive cells)\n",
    "    the orientation of the deposits changes from 45 to 90 to 160 from N to S\n",
    "We imagine \n",
    "Obviously this could be done externally (GIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>major</th>\n",
       "      <th>transverse</th>\n",
       "      <th>normal</th>\n",
       "      <th>bearing</th>\n",
       "      <th>dip</th>\n",
       "      <th>k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>150.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>-160</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>75.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>50.0</td>\n",
       "      <td>25.00</td>\n",
       "      <td>6000</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>180</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>160</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>170</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3000</td>\n",
       "      <td>400</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35</td>\n",
       "      <td>15</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>150.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>-160</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>75.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>50.0</td>\n",
       "      <td>25.00</td>\n",
       "      <td>6000</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>180</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>160</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>170</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3000</td>\n",
       "      <td>400</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>35</td>\n",
       "      <td>15</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>150.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>-160</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>75.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>50.0</td>\n",
       "      <td>25.00</td>\n",
       "      <td>6000</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>180</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>160</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>170</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3000</td>\n",
       "      <td>400</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>35</td>\n",
       "      <td>15</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     i   j   mean     sd  major  transverse  normal  bearing   dip  k\n",
       "0    3   3  100.0  50.00   3000         300      10      100  0.10  0\n",
       "1    3  18  150.0  50.00   3000         300      10     -160  0.10  0\n",
       "2   10  10   75.0  50.00   1500         100       5       45  0.50  0\n",
       "3   15  15   50.0  25.00   6000         400       5      180  0.05  0\n",
       "4   15  10  100.0  10.00   1500         100       5       90  0.50  0\n",
       "5   20  10   20.0  50.00   1500         100       5      160  0.50  0\n",
       "6   25   3    1.0   0.50   1500         100       5      170  0.10  0\n",
       "7   33   5    0.5   0.10   3000         400      10       70  0.10  0\n",
       "8   35  10    2.0   0.90   3000         300       1       90  0.01  0\n",
       "9   35  15    0.1   0.05   3000         200       1       90  0.01  0\n",
       "10   3   3  100.0  50.00   3000         300      10      100  0.10  1\n",
       "11   3  18  150.0  50.00   3000         300      10     -160  0.10  1\n",
       "12  10  10   75.0  50.00   1500         100       5       45  0.50  1\n",
       "13  15  15   50.0  25.00   6000         400       5      180  0.05  1\n",
       "14  15  10  100.0  10.00   1500         100       5       90  0.50  1\n",
       "15  20  10   20.0  50.00   1500         100       5      160  0.50  1\n",
       "16  25   3    1.0   0.50   1500         100       5      170  0.10  1\n",
       "17  33   5    0.5   0.10   3000         400      10       70  0.10  1\n",
       "18  35  10    2.0   0.90   3000         300       1       90  0.01  1\n",
       "19  35  15    0.1   0.05   3000         200       1       90  0.01  1\n",
       "20   3   3  100.0  50.00   3000         300      10      100  0.10  2\n",
       "21   3  18  150.0  50.00   3000         300      10     -160  0.10  2\n",
       "22  10  10   75.0  50.00   1500         100       5       45  0.50  2\n",
       "23  15  15   50.0  25.00   6000         400       5      180  0.05  2\n",
       "24  15  10  100.0  10.00   1500         100       5       90  0.50  2\n",
       "25  20  10   20.0  50.00   1500         100       5      160  0.50  2\n",
       "26  25   3    1.0   0.50   1500         100       5      170  0.10  2\n",
       "27  33   5    0.5   0.10   3000         400      10       70  0.10  2\n",
       "28  35  10    2.0   0.90   3000         300       1       90  0.01  2\n",
       "29  35  15    0.1   0.05   3000         200       1       90  0.01  2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp = pd.DataFrame({\n",
    "    'i': [3, 3, 10, 15, 15, 20, 25, 33, 35, 35], \n",
    "    'j': [3, 18, 10, 15, 10, 10, 3, 5, 10, 15],\n",
    "    'mean': [100, 150, 75, 50, 100, 20, 1, 0.5, 2, 0.1],\n",
    "    'sd': [50, 50, 50, 25, 10, 50, 0.5, 0.1, 0.9, 0.05],\n",
    "    'major': [3000, 3000, 1500, 6000, 1500, 1500, 1500, 3000, 3000, 3000],\n",
    "    'transverse': [300, 300, 100, 400, 100, 100, 100, 400, 300, 200],\n",
    "    'normal': [10, 10, 5, 5, 5, 5, 5, 10, 1, 1],\n",
    "    'bearing': [100, -160, 45, 180, 90, 160, 170, 70, 90, 90],\n",
    "    'dip': [0.1, 0.1, 0.5, 0.05, 0.5, 0.5, 0.1, 0.1, 0.01, 0.01]\n",
    "})\n",
    "cp = pd.concat([cp.assign(k=k) for k in range(3)], ignore_index=True)\n",
    "cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>major</th>\n",
       "      <th>transverse</th>\n",
       "      <th>normal</th>\n",
       "      <th>bearing</th>\n",
       "      <th>dip</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>875.0</td>\n",
       "      <td>9125.0</td>\n",
       "      <td>36.751535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>-160</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4625.0</td>\n",
       "      <td>9125.0</td>\n",
       "      <td>34.766945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>7375.0</td>\n",
       "      <td>35.071070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6000</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>180</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3875.0</td>\n",
       "      <td>6125.0</td>\n",
       "      <td>34.470650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>6125.0</td>\n",
       "      <td>34.882355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1500</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>160</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>4875.0</td>\n",
       "      <td>34.830705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   major  transverse  normal  bearing   dip       x       y          z\n",
       "0   3000         300      10      100  0.10   875.0  9125.0  36.751535\n",
       "1   3000         300      10     -160  0.10  4625.0  9125.0  34.766945\n",
       "2   1500         100       5       45  0.50  2625.0  7375.0  35.071070\n",
       "3   6000         400       5      180  0.05  3875.0  6125.0  34.470650\n",
       "4   1500         100       5       90  0.50  2625.0  6125.0  34.882355\n",
       "5   1500         100       5      160  0.50  2625.0  4875.0  34.830705"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp['x'] = cc[0][cp['i'], cp['j']]\n",
    "cp['y'] = cc[1][cp['i'], cp['j']]\n",
    "cp['z'] = cc[2][cp['k'], cp['i'], cp['j']]\n",
    "cp.loc[:5,['major','transverse', 'normal', 'bearing', 'dip', 'x','y','z']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp['name']=cp.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize the conceptual points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using first layer coordinates from 3-layer grid\n",
      "Plot saved to: temp_pst_from_nsaf\\conceptual_points.png\n"
     ]
    }
   ],
   "source": [
    "pu.plot_zones_with_conceptual_points(zones[0], cp, grid_coords, save_path=tmp_model_ws)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpolate the anisotropic variance tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a little tweak to bearing\n",
    "# normalization that preserves direction\n",
    "cp['bearing'] = cp.bearing % 360\n",
    "cp['bearing_rad'] = np.radians(cp.bearing)\n",
    "cp_tensors = nsaf.create_2d_tensors(cp.bearing_rad, cp.major, cp.transverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125.0</td>\n",
       "      <td>9875.0</td>\n",
       "      <td>36.951200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>375.0</td>\n",
       "      <td>9875.0</td>\n",
       "      <td>36.915405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>625.0</td>\n",
       "      <td>9875.0</td>\n",
       "      <td>36.839995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>875.0</td>\n",
       "      <td>9875.0</td>\n",
       "      <td>36.727160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1125.0</td>\n",
       "      <td>9875.0</td>\n",
       "      <td>36.604655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>3875.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>15.287420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>4125.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>15.327858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>4375.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>15.378074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>4625.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>15.475265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>4875.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>15.545507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x       y          z\n",
       "0      125.0  9875.0  36.951200\n",
       "1      375.0  9875.0  36.915405\n",
       "2      625.0  9875.0  36.839995\n",
       "3      875.0  9875.0  36.727160\n",
       "4     1125.0  9875.0  36.604655\n",
       "...      ...     ...        ...\n",
       "2395  3875.0   125.0  15.287420\n",
       "2396  4125.0   125.0  15.327858\n",
       "2397  4375.0   125.0  15.378074\n",
       "2398  4625.0   125.0  15.475265\n",
       "2399  4875.0   125.0  15.545507\n",
       "\n",
       "[2400 rows x 3 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 20\n"
     ]
    }
   ],
   "source": [
    "cp_coords = cp[['x', 'y']].values\n",
    "cp_tensors = nsaf.create_2d_tensors(cp.bearing_rad, cp.major, cp.transverse)\n",
    "geological_tensors = nsaf.interpolate_tensors(cp_coords, cp_tensors, grid_coords_2d,\n",
    "                                             zones=zones[0].copy(), method='idw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting 200 tensors out of 800\n",
      "Plotted 26 default tensors (black circles) and 174 ellipses\n",
      "Saved tensor visualization to temp_pst_from_nsaf\\intrpolated_tensors_None.png\n"
     ]
    }
   ],
   "source": [
    "pu.visualize_geological_tensors(geological_tensors, grid_coords_2d, shape=zones.shape[1:], zones=zones[0], save_path=tmp_model_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Smoothing values at geological boundaries...\n",
      "    Smoothed 490 points near boundaries\n"
     ]
    }
   ],
   "source": [
    "transform = 'log'\n",
    "cp_means = cp['mean'].values\n",
    "interp_means_2d = nsaf.tensor_aware_kriging(\n",
    "        cp_coords, cp_means, grid_coords_2d, geological_tensors,\n",
    "        variogram_model='exponential', sill=1.0, nugget=0.1,\n",
    "        background_value=np.mean(cp_means), max_search_radius=1e20,\n",
    "        min_points=3, transform=transform, min_value=1e-8, max_neighbors=4,\n",
    "        zones=zones[0]\n",
    "    )\n",
    "if zones is not None:\n",
    "    print(\"  Smoothing values at geological boundaries...\")\n",
    "    interp_means_2d = nsaf.create_boundary_modified_scalar(\n",
    "        interp_means_2d, zones[0],\n",
    "        transition_cells=3, mode='smooth'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15f9744ef00>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(interp_means_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Interpolating standard deviation...\n",
      "  Enhancing variance at geological boundaries...\n",
      "    Enhanced 490 points near boundaries\n"
     ]
    }
   ],
   "source": [
    "cp_sd = cp['sd'].values\n",
    "print(\"  Interpolating standard deviation...\")\n",
    "interp_sd_2d = nsaf.tensor_aware_kriging(\n",
    "    cp_coords, cp_sd, grid_coords_2d, geological_tensors,  # Use geological tensors\n",
    "    variogram_model='exponential', sill=1.0, nugget=0.1,\n",
    "    background_value=np.mean(cp_sd), max_search_radius=1e20,\n",
    "    min_points=3, transform=transform, min_value=1e-8, max_neighbors=4,\n",
    "    zones=zones[0]\n",
    ")\n",
    "if zones is not None:\n",
    "    print(\"  Enhancing variance at geological boundaries...\")\n",
    "    interp_sd_2d = nsaf.create_boundary_modified_scalar(\n",
    "        interp_sd_2d, zones[0],\n",
    "        peak_increase=1, transition_cells=3, mode='enhance'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining fields using log-normal formulation...\n"
     ]
    }
   ],
   "source": [
    "iids = np.random.randn(*zones[0].shape)\n",
    "correlated_noise_1d = nsaf.generate_correlated_noise_2d(grid_coords_2d, geological_tensors, iids)\n",
    "correlated_noise_2d = correlated_noise_1d.reshape(zones[0].shape)\n",
    "\n",
    "# Step 7: Combine fields with transform handling\n",
    "if transform == 'log':\n",
    "    print(\"  Combining fields using log-normal formulation...\")\n",
    "    # Convert to lognormal parameters\n",
    "    variance_2d = interp_sd_2d ** 2\n",
    "    mu_2d = np.log(interp_means_2d ** 2 / np.sqrt(variance_2d + interp_means_2d ** 2))\n",
    "    sigma_2d = np.sqrt(np.log(1 + variance_2d / interp_means_2d ** 2))\n",
    "    normal_field = mu_2d + correlated_noise_2d * sigma_2d\n",
    "    field_2d = np.exp(normal_field)\n",
    "else:\n",
    "    print(\"  Combining fields using normal formulation...\")\n",
    "    field_2d = interp_means_2d + correlated_noise_2d * interp_sd_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.log10(field_2d)*id_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate a `PstFrom` class instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_ws = \"freyberg6_template\"\n",
    "pf = pyemu.utils.PstFrom(original_d=tmp_model_ws, new_d=template_ws,\n",
    "                 remove_existing=True,\n",
    "                 longnames=True, \n",
    "                 zero_based=False,start_datetime=\"1-1-2018\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "So now that we have a `PstFrom` instance, but its just an empty container at this point, so we need to add some PEST interface \"observations\" and \"parameters\".  Let's start with observations using MODFLOW6 head.  These are stored in `heads.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(tmp_model_ws,\"heads.csv\"),index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main entry point for adding observations is (surprise) `PstFrom.add_observations()`.  This method works on the list-type observation output file.  We need to tell it what column is the index column (can be string if there is a header or int if no header) and then what columns contain quantities we want to monitor (e.g. \"observe\") in the control file - in this case we want to monitor all columns except the index column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_df = pf.add_observations(\"heads.csv\",insfile=\"heads.csv.ins\",index_cols=\"time\",\n",
    "                    use_cols=list(df.columns.values),prefix=\"hds\",)\n",
    "hds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it returned a dataframe with lots of useful info: the observation names that were formed (`obsnme`), the values that were read from `heads.csv` (`obsval`) and also some generic weights and group names.  At this point, no control file has been created, we have simply prepared to add this observations to the control file later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(template_ws) if f.endswith(\".ins\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!  We also have a PEST-style instruction file for those obs.\n",
    "\n",
    "Now lets do the same for SFR observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(tmp_model_ws, \"sfr.csv\"), index_col=0)\n",
    "sfr_df = pf.add_observations(\"sfr.csv\", insfile=\"sfr.csv.ins\", index_cols=\"time\", use_cols=list(df.columns.values))\n",
    "sfr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet as!  Now that we have some observations, let's add parameters!\n",
    "\n",
    "## Pilot points and `PyPestUtils`\n",
    "\n",
    "This notebook is mostly meant to demonstrate some advanced pilot point parameterization that is possible with `PyPestUtils`, so we will only focus on HK and VK pilot point parameters.  This is just to keep the example short.  In practice, please please please parameterize boundary conditions too!\n",
    "\n",
    "We start with simple pilot points and an isotropic variogram (bearing is irrelevant when anisotropy=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pyemu.geostats.ExpVario(contribution=1.0,a=5000,bearing=0,anisotropy=1)\n",
    "pp_gs = pyemu.geostats.GeoStruct(variograms=v, transform='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_gs.plot()\n",
    "print(\"spatial variogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the idomain array to use as a zone array - this keeps us from setting up parameters in inactive model cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib = m.dis.idomain[0].array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find HK files for the upper and lower model layers (assuming model layer 2 is a semi-confining unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_arr_files = [f for f in os.listdir(tmp_model_ws) if \"npf_k_\" in f and f.endswith(\".txt\") and \"layer2\" not in f]\n",
    "hk_arr_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_file = \"freyberg6.npf_k_layer1.txt\"\n",
    "tag = arr_file.split('.')[1].replace(\"_\",\"-\")\n",
    "pf.add_parameters(filenames=arr_file,par_type=\"pilotpoints\",\n",
    "                   par_name_base=tag,pargp=tag,zone_array=ib,\n",
    "                   upper_bound=10.,lower_bound=0.1,ult_ubound=100,ult_lbound=0.01,\n",
    "                   pp_options={\"pp_space\":3},geostruct=pp_gs, transform='log')\n",
    "#let's also add the resulting hk array that modflow sees as observations\n",
    "# so we can make easy plots later...\n",
    "pf.add_observations(arr_file,prefix=tag,\n",
    "                    obsgp=tag,zone_array=ib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with how `PstFrom` has worked historically, we handed off the process to solve for the factor file (which requires solving the kriging equations for each active node) to a pure python (well, with pandas and numpy).  This was ok for toy models, but hella slow for big ugly models.  If you look at the log entries above, you should see that the instead, `PstFrom` successfully handed off the solve to `PyPestUtils`, which is exponentially faster for big models.  sweet ez! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpl_files = [f for f in os.listdir(template_ws) if f.endswith(\".tpl\")]\n",
    "tpl_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(template_ws,tpl_files[0]),'r') as f:\n",
    "    for _ in range(2):\n",
    "        print(f.readline().strip())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So those might look like pretty redic parameter names, but they contain heaps of metadata to help you post process things later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So those are you standard pilot points for HK in layer 1 - same as it ever was..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geostatistical hyper-parameters using \"conceptual points\"\n",
    "\n",
    "For the HK layer 1 pilot points, we used a standard geostatistical structure - the ever popular exponential variogram.  But what if the properties that define that variogram were themselves uncertain?  Like what if the anisotropy ellipse varied in space across the model domain?  \n",
    "\n",
    "One way to tackle this is to assign hyper parameters at points, a tensor describing spatial correlation structure at each point. This makes sense when you have data (or \"geology\" = \"beer and guessing\") at various points to inform the correlation structure. We then use tensor interpolation to fill in the spatial structure where \"beer and guessing\" is missing.\n",
    "\n",
    "The field will be calculated as:\n",
    "\n",
    "field = interp_means + correlated_noise * 10**np.sqrt(interp_variances)\n",
    "\n",
    "The above gives us the spatially varying correlation structure to be convolved with white noise to give the correlated_noise, but the mean and variance of the field is also uncertain (uncertain uncertainty). The field is cacluLet's start with those using the familar pilot point scheme, although they could be added for each conceptual point instead. First describe how the mean and variance are expected to change (we will keep it simple):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_v = pyemu.geostats.ExpVario(contribution=1, a=5000)\n",
    "variance_gs = pyemu.geostats.GeoStruct(variograms=variance_v)\n",
    "mean_v = pyemu.geostats.ExpVario(contribution=1,a=5000)\n",
    "mean_gs = pyemu.geostats.GeoStruct(variograms=mean_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mean\n",
    "arr_file = \"freyberg6.npf_k33_layer3.txt\"\n",
    "tag = arr_file.split('.')[1].replace(\"_\",\"-\")\n",
    "pf.add_parameters(filenames=arr_file,par_type=\"pilotpoints\",\n",
    "                   par_name_base=tag,pargp=tag,zone_array=ib,\n",
    "                   upper_bound=10.,lower_bound=0.1,ult_ubound=100,ult_lbound=0.01,\n",
    "                   pp_options={\"pp_space\":3},geostruct=mean_gs, transform='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variance\n",
    "arr_file = \"freyberg6.npf_k33_layer3.txt\"\n",
    "tag = arr_file.split('.')[1].replace(\"_\",\"-\")\n",
    "pf.add_parameters(filenames=arr_file,par_type=\"pilotpoints\",\n",
    "                   par_name_base=tag,pargp=tag,zone_array=ib,\n",
    "                   upper_bound=10./2,lower_bound=0.1*2,ult_ubound=10,ult_lbound=0.1,\n",
    "                   pp_options={\"pp_space\":3},geostruct=mean_gs, transform='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's also add the resulting hk array that modflow sees as observations\n",
    "# so we can make easy plots later...\n",
    "pf.add_observations(arr_file,prefix=tag,\n",
    "                    obsgp=tag,zone_array=ib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the conceptual point hyper parameters from our friendly geologists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinary_kriging(cp_coords, cp_values, grid_coords,\n",
    "                     variogram_model='exponential', range_param=10000,\n",
    "                     sill=1.0, nugget=0.1, background_value=0.0,\n",
    "                     max_search_radius=1e20, min_points=1,\n",
    "                     transform=None, min_value=1e-8):\n",
    "    \"\"\"\n",
    "    Proper Ordinary Kriging with:\n",
    "    - Variogram model selection\n",
    "    - Background value fallback\n",
    "    - Search radius limitation\n",
    "    \"\"\"\n",
    "    n_grid = len(grid_coords)\n",
    "    n_control = len(cp_coords)\n",
    "    interp_values = np.full(n_grid, background_value)\n",
    "\n",
    "    # Variogram function\n",
    "    def variogram(h):\n",
    "        if variogram_model == 'exponential':\n",
    "            return nugget + (sill - nugget) * (1 - np.exp(-h / range_param))\n",
    "        elif variogram_model == 'gaussian':\n",
    "            return nugget + (sill - nugget) * (1 - np.exp(-(h ** 2) / (range_param ** 2)))\n",
    "        else:  # spherical\n",
    "            return np.where(h <= range_param,\n",
    "                            nugget + (sill - nugget) * (1.5 * h / range_param - 0.5 * (h / range_param) ** 3),\n",
    "                            sill)\n",
    "\n",
    "    for i in range(n_grid):\n",
    "        # Distances from this grid point to all control points\n",
    "        distances = np.linalg.norm(cp_coords - grid_coords[i], axis=1)\n",
    "        nearby = distances <= max_search_radius\n",
    "        n_nearby = np.sum(nearby)\n",
    "\n",
    "        if n_nearby < min_points:\n",
    "            continue  # Keeps background_value\n",
    "\n",
    "        # Build OK system: [C 1; 1^T 0] * [weights; mu] = [c; 1]\n",
    "        C = np.zeros((n_nearby + 1, n_nearby + 1))\n",
    "        c = np.zeros(n_nearby + 1)\n",
    "\n",
    "        # Fill covariance matrix\n",
    "        for j in range(n_nearby):\n",
    "            for k in range(n_nearby):\n",
    "                h = np.linalg.norm(cp_coords[nearby][j] - cp_coords[nearby][k])\n",
    "                C[j, k] = variogram(h)\n",
    "            # Last row/column (unbiasedness constraint)\n",
    "            C[j, -1] = 1\n",
    "            C[-1, j] = 1\n",
    "            # Right-hand side vector\n",
    "            h = distances[nearby][j]\n",
    "            c[j] = variogram(h)\n",
    "        c[-1] = 1  # Constraint\n",
    "\n",
    "        # Solve\n",
    "        if transform == 'log':\n",
    "            if min_value is None:\n",
    "                # Auto-choose based on data scale\n",
    "                positive_values = cp_values[cp_values > 0]\n",
    "                min_value = np.min(positive_values) * 0.01  # 1% of smallest positive value\n",
    "            cp_values_transformed = np.log10(np.maximum(cp_values, min_value))\n",
    "        else:\n",
    "            cp_values_transformed = cp_values.copy()\n",
    "\n",
    "        try:\n",
    "            weights = solve(C, c)[:-1]\n",
    "            interp_values[i] = np.sum(weights * cp_values_transformed[nearby])\n",
    "        except:\n",
    "            weights = np.exp(-distances[nearby] / range_param)\n",
    "            interp_values[i] = np.sum(weights * cp_values_transformed[nearby]) / np.sum(weights)\n",
    "\n",
    "    # After the loop, backtransform all at once:\n",
    "    if transform == 'log':\n",
    "        interp_values = 10 ** (interp_values)\n",
    "\n",
    "    return interp_values\n",
    "\n",
    "def generate_single_layer(control_points, interp_means, interp_variances, grid_coords_2d):\n",
    "    \"\"\"Generate field for a single 2D layer\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_grid = len(grid_coords_2d)\n",
    "    cp_coords = control_points[['x', 'y']].values\n",
    "    \n",
    "    # Interpolate correlation tensors\n",
    "    print(\"  Interpolating correlation tensors...\")\n",
    "    cp_tensors = create_2d_tensors(control_points)\n",
    "    interp_tensors = interpolate_tensors_2d(cp_coords, cp_tensors, grid_coords_2d)\n",
    "\n",
    "    # Generate correlated noise\n",
    "    print(\"  Generating correlated noise...\")\n",
    "    white_noise = np.random.randn(len(grid_coords_2d))\n",
    "    correlated_noise = generate_correlated_noise_2d(grid_coords_2d, interp_tensors, white_noise)\n",
    "\n",
    "    # Combine\n",
    "    field = interp_means + correlated_noise * 10**np.sqrt(interp_variances)\n",
    "\n",
    "    return field\n",
    "\n",
    "\n",
    "def generate_correlated_noise_2d(grid_coords, tensors, white_noise,\n",
    "                                 n_neighbors=50, radius_multiplier=3.0,\n",
    "                                 anisotropy_strength=1.0, debug=False):\n",
    "    \"\"\"\n",
    "    Generate spatially correlated noise respecting tensor anisotropy directions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    grid_coords : array_like, shape (n_points, 2)\n",
    "        Grid coordinates [x, y]\n",
    "    tensors : array_like, shape (n_points, 2, 2)\n",
    "        Anisotropy tensors for each grid point\n",
    "    white_noise : array_like, shape (n_points,)\n",
    "        Input white noise to be correlated\n",
    "    n_neighbors : int, default=10\n",
    "        Number of neighbors to use for correlation\n",
    "    radius_multiplier : float, default=3.0\n",
    "        Multiplier for correlation radius (not used when n_neighbors is set)\n",
    "    anisotropy_strength : float, default=2.0\n",
    "        Strength of anisotropic correlation (higher = more directional)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    correlated_noise : array_like, shape (n_points,)\n",
    "        Spatially correlated noise\n",
    "    \"\"\"\n",
    "    n_points = len(grid_coords)\n",
    "    correlated_noise = np.zeros(n_points)\n",
    "\n",
    "    for i in range(n_points):\n",
    "        # Diagonalize tensor to get principal axes and correlation lengths\n",
    "        evals, evecs = np.linalg.eigh(tensors[i])\n",
    "        if evals[1] < evals[0]:\n",
    "            evals, evecs = evals[::-1], evecs[:, ::-1]\n",
    "\n",
    "        # some debuggery\n",
    "        if debug:\n",
    "            print(f\"Eigenvalue 0: {evals[0]}\")\n",
    "            print(f\"Eigenvector 0: {evecs[:, 0]}\")  # First COLUMN\n",
    "            print(f\"Eigenvalue 1: {evals[1]}\")\n",
    "            print(f\"Eigenvector 1: {evecs[:, 1]}\")  # Second COLUMN\n",
    "            # Find which eigenvalue is larger\n",
    "            max_idx = np.argmax(np.abs(evals))\n",
    "            principal_direction = evecs[:, max_idx]\n",
    "            print(f\"Principal direction vector: {principal_direction}\")\n",
    "            angle = np.arctan2(principal_direction[1], principal_direction[0])\n",
    "            angle_degrees = np.degrees(angle)\n",
    "            print(f\"angle: {angle_degrees}\")\n",
    "\n",
    "        major_corr_length = np.sqrt(evals[1])  # Length along major axis\n",
    "        minor_corr_length = np.sqrt(evals[0])  # Length along minor axis\n",
    "\n",
    "        # Create anisotropic transform\n",
    "        # Stretch major axis to make correlation stronger in that direction\n",
    "        scale_for_minor_axis = minor_corr_length / anisotropy_strength  # Compress minor\n",
    "        scale_for_major_axis = major_corr_length * anisotropy_strength  # Stretch major\n",
    "\n",
    "        transform = evecs @ np.diag([scale_for_minor_axis, scale_for_major_axis])\n",
    "\n",
    "        # Transform all relative positions to anisotropic space\n",
    "        all_dx = grid_coords - grid_coords[i]\n",
    "        inv_transform = np.linalg.inv(transform)\n",
    "        dx_transformed = all_dx @ inv_transform.T\n",
    "        aniso_distances = np.linalg.norm(dx_transformed, axis=1)\n",
    "        actual_distances = np.linalg.norm(all_dx, axis=1)\n",
    "\n",
    "        # Select neighbors based on anisotropic distance, closer in warped space\n",
    "        if n_neighbors:\n",
    "            neighbor_indices = np.argsort(aniso_distances)[:min(n_neighbors, n_points)]\n",
    "        else:\n",
    "            max_dist = radius_multiplier * major_corr_length\n",
    "            neighbor_indices = np.where(aniso_distances <= max_dist)[0]\n",
    "\n",
    "        # Compute correlation weights using anisotropic distances\n",
    "        weighted_sum = 0.0\n",
    "        sum_weights = 0.0\n",
    "\n",
    "        for j in neighbor_indices:\n",
    "            # Use major correlation length as decay scale\n",
    "            weight = np.exp(-(actual_distances[j]/scale_for_major_axis)**2)\n",
    "            weighted_sum += weight * white_noise[j]\n",
    "            sum_weights += weight\n",
    "\n",
    "        # Normalize\n",
    "        if sum_weights > 1e-10:\n",
    "            correlated_noise[i] = weighted_sum / sum_weights\n",
    "        else:\n",
    "            correlated_noise[i] = white_noise[i]\n",
    "\n",
    "    return correlated_noise\n",
    "\n",
    "def create_2d_tensors(control_points):\n",
    "    # Fixed tensor creation (swap major/minor in diagonal matrix)\n",
    "    n = len(control_points)\n",
    "    tensors = np.zeros((n, 2, 2))\n",
    "\n",
    "    for i in range(n):\n",
    "        major = control_points.iloc[i]['major']\n",
    "        minor = control_points.iloc[i]['major'] / control_points.iloc[i]['ratio']\n",
    "        bearing = control_points.iloc[i]['bearing'] # - 90 # assume bearing has N=0\n",
    "\n",
    "        theta = np.radians(bearing)\n",
    "        R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                      [np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "        # Fixed: Put minor first, major second\n",
    "        S = np.diag([minor ** 2, major ** 2])\n",
    "        tensors[i] = R @ S @ R.T\n",
    "\n",
    "    return tensors\n",
    "\n",
    "\n",
    "def interpolate_tensors_2d(cp_coords, cp_tensors, grid_coords):\n",
    "    \"\"\"Interpolate 2x2 tensors using log-Euclidean approach\"\"\"    \n",
    "    from scipy.linalg import logm, expm, solve\n",
    "    n_grid = len(grid_coords)\n",
    "    interp_tensors = np.zeros((n_grid, 2, 2))\n",
    "\n",
    "    # Convert to log space\n",
    "    log_tensors = np.array([logm(t) for t in cp_tensors])\n",
    "\n",
    "    # Interpolate each component\n",
    "    for i in range(2):\n",
    "        for j in range(i, 2):\n",
    "            values = log_tensors[:, i, j].real\n",
    "            bg_value = np.mean(values)  # Use mean of control points\n",
    "            interp_values = ordinary_kriging(cp_coords, values, grid_coords, \n",
    "                                       background_value=bg_value)\n",
    "            # interp_values = exponential_variogram_interpolate(cp_coords, values, grid_coords)\n",
    "            # interp_values = idw_interpolate(cp_coords, values, grid_coords)\n",
    "            interp_tensors[:, i, j] = interp_values\n",
    "            if i != j:\n",
    "                interp_tensors[:, j, i] = interp_values\n",
    "\n",
    "    # Convert back from log space\n",
    "    for i in range(n_grid):\n",
    "        interp_tensors[i] = expm(interp_tensors[i])\n",
    "\n",
    "    # # Basic ellipse visualization (most intuitive)\n",
    "    # fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    # visualize_tensors_as_ellipses(grid_coords, interp_tensors,\n",
    "    #                               subsample=20,  # Plot every 20th tensor\n",
    "    #                               scale_factor=0.5,\n",
    "    #                               ax=ax)\n",
    "\n",
    "    # # Complete analysis\n",
    "    # plot_tensor_field_analysis(grid_coords, interp_tensors, cp_coords)\n",
    "\n",
    "    return interp_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_means = np.loadtxt(os.path.join(org_model_ws,arr_file)).flatten()\n",
    "interp_variances = np.loadtxt(os.path.join(org_model_ws,arr_file)).flatten()/10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_centers = m.modelgrid.xcellcenters.flatten()\n",
    "y_centers = m.modelgrid.ycellcenters.flatten()\n",
    "grid_coords = np.column_stack([x_centers,y_centers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptual_points = pd.DataFrame({'x': [3, 3, 3, 3, 5, 7, 15, 20, 25, 35],\n",
    "                                  'y': [3, 5, 10, 15, 5, 12, 10, 15, 3, 10],\n",
    "                                  'bearing': [-40, -30, -20, -20, -30, -40, 0, 0, 10 ,200],\n",
    "                                  'major': [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 2000],\n",
    "                                  'ratio': [5, 5, 2, 2, 2, 2, 2, 5, 5, 2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptual_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = generate_single_layer(conceptual_points, interp_means, interp_variances, grid_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(field.reshape(40,20))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the control file, pest interface files, and forward run script\n",
    "At this point, we have some parameters and some observations, so we can create a control file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.mod_sys_cmds.append(\"mf6\")\n",
    "pf.pre_py_cmds.insert(0,\"import sys\")\n",
    "pf.pre_py_cmds.insert(1,\"sys.path.append(os.path.join('..','..','..','pypestutils'))\")\n",
    "pst = pf.build_pst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [print(line.rstrip()) for line in open(os.path.join(template_ws,\"forward_run.py\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting initial parameter bounds and values\n",
    "\n",
    "Here is some gory detail regarding defining the hyper parameters for both layer 3 HK and layer 2 VK..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the initial and bounds for the fill values\n",
    "par = pst.parameter_data\n",
    "\n",
    "apar = par.loc[par.pname.str.contains(\"aniso\"),:]\n",
    "bpar = par.loc[par.pname.str.contains(\"bearing\"), :]\n",
    "par.loc[apar.parnme.str.contains(\"layer3\").index,\"parval1\"] = 3\n",
    "par.loc[apar.parnme.str.contains(\"layer3\").index,\"parlbnd\"] = 1\n",
    "par.loc[apar.parnme.str.contains(\"layer3\").index,\"parubnd\"] = 5\n",
    "\n",
    "par.loc[apar.parnme.str.contains(\"layer2\").index,\"parval1\"] = 2\n",
    "par.loc[apar.parnme.str.contains(\"layer2\").index,\"parlbnd\"] = 0\n",
    "par.loc[apar.parnme.str.contains(\"layer2\").index,\"parubnd\"] = 4\n",
    "\n",
    "par.loc[bpar.parnme.str.contains(\"layer3\").index,\"parval1\"] = 0\n",
    "par.loc[bpar.parnme.str.contains(\"layer3\").index,\"parlbnd\"] = -20\n",
    "par.loc[bpar.parnme.str.contains(\"layer3\").index,\"parubnd\"] = 20\n",
    "\n",
    "par.loc[bpar.parnme.str.contains(\"layer2\").index,\"parval1\"] = 0\n",
    "par.loc[bpar.parnme.str.contains(\"layer2\").index,\"parlbnd\"] = 20\n",
    "par.loc[bpar.parnme.str.contains(\"layer2\").index,\"parubnd\"] = 40\n",
    "\n",
    "cat1par = par.loc[par.apply(lambda x: x.threshcat==\"0\" and x.usecol==\"threshfill\",axis=1),\"parnme\"]\n",
    "cat2par = par.loc[par.apply(lambda x: x.threshcat == \"1\" and x.usecol == \"threshfill\", axis=1), \"parnme\"]\n",
    "assert cat1par.shape[0] == 1\n",
    "assert cat2par.shape[0] == 1\n",
    "\n",
    "cat1parvk = [p for p in cat1par if \"k:1\" in p]\n",
    "cat2parvk = [p for p in cat2par if \"k:1\" in p]\n",
    "for lst in [cat2parvk,cat1parvk]:\n",
    "    assert len(lst) > 0\n",
    "\n",
    "#these are the values that will fill the two categories of VK - \n",
    "# one is low (clay) and one is high (sand - the windows)\n",
    "par.loc[cat1parvk, \"parval1\"] = 0.0001\n",
    "par.loc[cat1parvk, \"parubnd\"] = 0.01\n",
    "par.loc[cat1parvk, \"parlbnd\"] = 0.000001\n",
    "par.loc[cat1parvk, \"partrans\"] = \"log\"\n",
    "par.loc[cat2parvk, \"parval1\"] = 0.1\n",
    "par.loc[cat2parvk, \"parubnd\"] = 1\n",
    "par.loc[cat2parvk, \"parlbnd\"] = 0.01\n",
    "par.loc[cat2parvk, \"partrans\"] = \"log\"\n",
    "\n",
    "\n",
    "cat1par = par.loc[par.apply(lambda x: x.threshcat == \"0\" and x.usecol == \"threshproportion\", axis=1), \"parnme\"]\n",
    "cat2par = par.loc[par.apply(lambda x: x.threshcat == \"1\" and x.usecol == \"threshproportion\", axis=1), \"parnme\"]\n",
    "\n",
    "assert cat1par.shape[0] == 1\n",
    "assert cat2par.shape[0] == 1\n",
    "\n",
    "#these are the proportions of clay and sand in the resulting categorical array\n",
    "#really under the hood, only the first one is used, so we can fix the other.\n",
    "par.loc[cat1par, \"parval1\"] = 0.95\n",
    "par.loc[cat1par, \"parubnd\"] = 1.0\n",
    "par.loc[cat1par, \"parlbnd\"] = 0.9\n",
    "par.loc[cat1par,\"partrans\"] = \"none\"\n",
    "\n",
    "# since the apply method only looks that first proportion, we can just fix this one\n",
    "par.loc[cat2par, \"parval1\"] = 1\n",
    "par.loc[cat2par, \"parubnd\"] = 1\n",
    "par.loc[cat2par, \"parlbnd\"] = 1\n",
    "par.loc[cat2par,\"partrans\"] = \"fixed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a prior parameter ensemble, then run and viz a real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(122346)\n",
    "pe = pf.draw(num_reals=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.to_csv(os.path.join(template_ws,\"prior.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = 0\n",
    "pst_name = \"real_{0}.pst\".format(real)\n",
    "pst.parameter_data.loc[pst.adj_par_names,\"parval1\"] = pe.loc[real,pst.adj_par_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = 0\n",
    "pst.write(os.path.join(pf.new_d,pst_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.run(\"pestpp-ies {0}\".format(pst_name),cwd=pf.new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.set_res(os.path.join(pf.new_d,pst_name.replace(\".pst\",\".base.rei\")))\n",
    "res = pst.res\n",
    "obs = pst.observation_data\n",
    "grps = [o for o in obs.obgnme.unique() if o.startswith(\"npf\") and \"result\" not in o and \"aniso\" not in o]\n",
    "grps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gobs = obs.loc[obs.obgnme.isin(grps),:].copy()\n",
    "gobs[\"i\"] = gobs.i.astype(int)\n",
    "gobs[\"j\"] = gobs.j.astype(int)\n",
    "gobs[\"k\"] = gobs.obgnme.apply(lambda x: int(x.split('-')[2].replace(\"layer\",\"\")) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk = gobs.k.unique()\n",
    "uk.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in uk:\n",
    "    kobs = gobs.loc[gobs.k==k,:]\n",
    "    ug = kobs.obgnme.unique()\n",
    "    ug.sort()\n",
    "    fig,axes = plt.subplots(1,4,figsize=(20,6))\n",
    "    axes = np.atleast_1d(axes)\n",
    "    for ax in axes:\n",
    "        ax.set_frame_on(False)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([])\n",
    "    for g,ax in zip(ug,axes):\n",
    "        gkobs = kobs.loc[kobs.obgnme==g,:]\n",
    "        \n",
    "        arr = np.zeros_like(top_arr)\n",
    "        arr[gkobs.i,gkobs.j] = res.loc[gkobs.obsnme,\"modelled\"].values\n",
    "        ax.set_aspect(\"equal\")\n",
    "        label = \"\"\n",
    "        if \"bearing\" not in g and \"aniso\" not in g:\n",
    "            arr = np.log10(arr)\n",
    "            label = \"$log_{10}$\"\n",
    "        cb = ax.imshow(arr)\n",
    "        plt.colorbar(cb,ax=ax,label=label)\n",
    "        ax.set_title(\"layer: {0} group: {1}\".format(k+1,g),loc=\"left\",fontsize=15)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stunning isn't it?!  There is clearly a lot subjectivity in the form of defining the prior for the hyper parameters required to use these non-stationary geostats, but they do afford more opportunities to express (stochastic) expert knowledge.  To be honest, there was a lot of experimenting with this notebook to get these figures to look this way - playing with variograms and parameter initial values and bounds a lot.  You encouraged to do the same!  scroll back up, change things, and \"restart kernel and run all\" - this will help build some better intution, promise...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
